import streamlit as st 
import tempfile
from openai import OpenAI
from io import BytesIO
from dotenv import load_dotenv
from pydub import AudioSegment

st.title("Speech Transcribing")

load_dotenv(override=True)
llm = OpenAI()

# Message state 
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages in the state
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])

audio_value = st.audio_input("Push the mic button and start speaking...")

if audio_value:
    audio_bytes = audio_value.read()

    # Save the uploaded audio to a temporary file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        tmp_file.write(audio_bytes) 
        tmp_file_path = tmp_file.name

    # Transcribe using OpenAI Whisper via GPT-4o endpoint
    with open(tmp_file_path, "rb") as audio_file:
        with st.spinner("Transcribing..."):
            transcript = llm.audio.transcriptions.create(
                model="gpt-4o-transcribe",  # OpenAI uses Whisper for transcription
                file=audio_file
            )

    # Check and fix the transcription using LLM 
    fixed_transcription = llm.responses.create(
        model = "gpt-4.1-mini",
        instructions="Correct the message based on context. The message is generated by speech-to-text system so it may contain some error. Fix the words based on context. You do not need to correct the grammar. Answer with the correct sentence.",
        input= "Context:\n" + "\n".join(["Role: " + m["role"] + ", message: " + m['content'] for m in st.session_state.messages]) + "Correct this sentence: " + transcript.text
    )

    with st.chat_message("human"):
        st.markdown(fixed_transcription.output[-1].content[-1].text)

    st.session_state.messages.append({"role":"user", "content":transcript.text})

    with st.chat_message("assistant"):
        stream = llm.chat.completions.create(
                model = "gpt-4.1-nano",
                messages= [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                stream=True,
            )
        response = st.write_stream(stream) 
        st.session_state.messages.append({"role":"assistant", "content": response})
        
        speech_file_path = "./speech/temp.mp3"
        with llm.audio.speech.with_streaming_response.create(
                model= "gpt-4o-mini-tts",
                voice= "coral",
                input= str(response),
                instructions= "Speak in a formal and strong.",
            ) as tts:
                tts.stream_to_file(speech_file_path)
        
        st.audio(speech_file_path)




